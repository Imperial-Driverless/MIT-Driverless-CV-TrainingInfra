{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkecJ56hacfI"
      },
      "source": [
        "# How to Train Your Own Cone Detection Networks\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/22118253/70957091-fe06a480-2042-11ea-8c06-0fcc549fc19a.png\">\n",
        "\n",
        "In this notebook, we will demonstrate \n",
        "- how to train your own YOLOv3-based traffic cone detection network and do inference on a video.\n",
        "\n",
        "**[Accurate Low Latency Visual Perception for Autonomous Racing: Challenges Mechanisms and Practical Solutions](https://github.com/mit-han-lab/once-for-all)** is an accurate low latency visual perception system introduced by Kieran Strobel, Sibo Zhu, Raphael Chang, and Skanda Koppula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Wdys-racfJ"
      },
      "source": [
        "## 1. Preparation\n",
        "Let's first install all the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8qZ7EQQacfK",
        "outputId": "a65d4d6c-71dc-4a1a-9a4e-70a0569f316e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Installing PyTorch...\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Installing torchvision...\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.2.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0+cu113)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Installing numpy...\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Installing tqdm (progress bar) ...\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Installing matplotlib...\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Installing Tensorboard\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.5\n",
            "Installing all the other required packages once for all\n",
            "python3: can't open file 'setup.py': [Errno 2] No such file or directory\n",
            "Installing video editor\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "! sudo apt install unzip\n",
        "print('Installing PyTorch...')\n",
        "! pip3 install torch \n",
        "print('Installing torchvision...')\n",
        "! pip3 install torchvision \n",
        "print('Installing numpy...')\n",
        "! pip3 install numpy \n",
        "# tqdm is a package for displaying a progress bar.\n",
        "print('Installing tqdm (progress bar) ...')\n",
        "! pip3 install tqdm \n",
        "print('Installing matplotlib...')\n",
        "! pip3 install matplotlib \n",
        "print('Installing Tensorboard')\n",
        "! pip3 install tensorboardx\n",
        "print('Installing all the other required packages once for all')\n",
        "! sudo python3 setup.py install\n",
        "print('Installing video editor')\n",
        "! sudo apt install ffmpeg -y "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi1QahgJacfK"
      },
      "source": [
        "Let' s clone our repo first..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi7PlETfacfL",
        "outputId": "2eb98fb7-9bcb-4758-ab00-bf919b5ec9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MIT-Driverless-CV-TrainingInfra'...\n",
            "remote: Enumerating objects: 934, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 934 (delta 30), reused 15 (delta 8), pack-reused 881\u001b[K\n",
            "Receiving objects: 100% (934/934), 9.90 MiB | 33.46 MiB/s, done.\n",
            "Resolving deltas: 100% (610/610), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone -b crop-image https://github.com/Imperial-Driverless/MIT-Driverless-CV-TrainingInfra.git\n",
        "\n",
        "! mv MIT-Driverless-CV-TrainingInfra/CVC-YOLOv3/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rq_4u6racfL"
      },
      "source": [
        "Before we start training, let's download the Cone Detection dataset and the corresponding label and intial training weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW8CaWKpacfL",
        "outputId": "fc4a93ab-0261-4341-9247-88e2d3a6d114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Training Dataset\n",
            "--2022-05-06 17:28:42--  https://storage.googleapis.com/mit-driverless-open-source/YOLO_Dataset.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.128, 142.250.141.128, 2607:f8b0:4023:c03::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-06 17:28:42 ERROR 403: Forbidden.\n",
            "\n",
            "unzip:  cannot find or open YOLO_Dataset.zip, YOLO_Dataset.zip.zip or YOLO_Dataset.zip.ZIP.\n",
            "mv: cannot stat 'YOLO_Dataset': No such file or directory\n",
            "Downloading YOLOv3 Sample Weights\n",
            "--2022-05-06 17:28:42--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.128, 142.250.141.128, 2607:f8b0:4023:c03::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-06 17:28:42 ERROR 403: Forbidden.\n",
            "\n",
            "Downloading Training and Validation Label\n",
            "--2022-05-06 17:28:42--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.128, 142.250.141.128, 2607:f8b0:4023:c03::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-06 17:28:43 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-06 17:28:43--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train_mini_yolo.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.128, 142.250.141.128, 2607:f8b0:4023:c03::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-06 17:28:43 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-06 17:28:43--  https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate_mini_yolo.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.128, 142.250.141.128, 2607:f8b0:4023:c0b::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-06 17:28:43 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(\"Downloading Training Dataset\")\n",
        "! wget https://storage.googleapis.com/mit-driverless-open-source/YOLO_Dataset.zip\n",
        "! unzip -q YOLO_Dataset.zip\n",
        "! mv YOLO_Dataset dataset/ && rm YOLO_Dataset.zip\n",
        "print(\"Downloading YOLOv3 Sample Weights\")\n",
        "! wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/sample-yolov3.weights \n",
        "print(\"Downloading Training and Validation Label\")\n",
        "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/all.csv && cd ..\n",
        "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/train_mini_yolo.csv && mv train_mini_yolo.csv train.csv && cd ..\n",
        "! cd dataset/ && wget https://storage.googleapis.com/mit-driverless-open-source/yolov3-training/validate_mini_yolo.csv && mv validate_mini_yolo.csv validate.csv && cd ..\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get required data"
      ],
      "metadata": {
        "id": "WKXJhS6-c6Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Downloading Training Dataset\"\n",
        "! gsutil cp -p gs://mit-driverless-open-source/YOLO_Dataset.zip ./dataset/\n",
        "! unzip dataset/YOLO_Dataset.zip -d ./dataset/\n",
        "! rm YOLO_Dataset.zip"
      ],
      "metadata": {
        "id": "JhLU82sSc9BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Downloading YOLOv3 pretrained Weights\"\n",
        "! gsutil cp -p  gs://mit-driverless-open-source/pretrained_yolo.weights ./yolo_weights/"
      ],
      "metadata": {
        "id": "59ErCYFI1A00",
        "outputId": "4a1ccdbe-a0b0-4a62-cd28-2c83086585e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading YOLOv3 pretrained Weights\n",
            "Copying gs://mit-driverless-open-source/pretrained_yolo.weights...\n",
            "\\ [1 files][236.5 MiB/236.5 MiB]                                                \n",
            "Operation completed over 1 objects/236.5 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/yolo_weights.zip /content/yolo_weights\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/yolo_weights.zip\")"
      ],
      "metadata": {
        "id": "_4pydU8NhPSc",
        "outputId": "1624471e-e565-45d7-a2b9-73ec853be5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/yolo_weights/ (stored 0%)\n",
            "  adding: content/yolo_weights/pretrained_yolo.weights (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_72e14b46-0584-4cc7-98b6-3d92476cfb54\", \"yolo_weights.zip\", 229985694)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"Downloading Training and Validation Label\"\n",
        "! gsutil cp -p gs://mit-driverless-open-source/yolov3-training/all.csv ./dataset/\n",
        "! gsutil cp -p gs://mit-driverless-open-source/yolov3-training/train.csv ./dataset/\n",
        "! gsutil cp -p gs://mit-driverless-open-source/yolov3-training/validate.csv ./dataset/"
      ],
      "metadata": {
        "id": "dmcqvaph1EyU",
        "outputId": "4c97d3da-11a2-433b-8564-54955b53b7e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Training and Validation Label\n",
            "Copying gs://mit-driverless-open-source/yolov3-training/all.csv...\n",
            "/ [1 files][  2.4 MiB/  2.4 MiB]                                                \n",
            "Operation completed over 1 objects/2.4 MiB.                                      \n",
            "Copying gs://mit-driverless-open-source/yolov3-training/train.csv...\n",
            "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n",
            "Copying gs://mit-driverless-open-source/yolov3-training/validate.csv...\n",
            "/ [1 files][350.3 KiB/350.3 KiB]                                                \n",
            "Operation completed over 1 objects/350.3 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr7l6X80acfM"
      },
      "source": [
        "## 2. Using Pretrained YOLOv3 Weights File to Start Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv4SLVTvacfM"
      },
      "source": [
        "First, import all the packages used in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j0_WnJvYacfM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import tempfile\n",
        "import time\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "import math\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from models import Darknet\n",
        "from utils.datasets import ImageLabelDataset\n",
        "from utils.utils import model_info, print_args, Logger, visualize_and_save_to_local,xywh2xyxy\n",
        "from yolo_tutorial_util import run_epoch\n",
        "import validate\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "##### section for all random seeds #####\n",
        "torch.manual_seed(2)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "########################################\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if cuda else 'cpu')\n",
        "num_cpu = multiprocessing.cpu_count() if cuda else 0\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.synchronize()\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px4VHbD3acfM"
      },
      "source": [
        "Successfully imported all packages and configured random seed to 17!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVntk1J-acfN"
      },
      "source": [
        "Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bi_I83dmacfN"
      },
      "outputs": [],
      "source": [
        "# Training Related Config\n",
        "batch_size = int(5)\n",
        "optimizer_pick = \"Adam\"\n",
        "model_cfg = \"model_cfg/yolo_baseline.cfg\"\n",
        "weights_path = \"sample-yolov3.weights\"\n",
        "output_path = \"automatic\"\n",
        "dataset_path = \"dataset/YOLO_Dataset/\"\n",
        "num_epochs = int(2) # Set them to 2048 during full dataset training\n",
        "num_steps = 8388608\n",
        "checkpoint_interval = int(1) # How often you want to get evaluation metric during training\n",
        "val_tolerance = int(3)\n",
        "min_epochs = int(3)\n",
        "\n",
        "# Dataloader Related Config\n",
        "data_aug = False # toggle for image augmentation\n",
        "blur = False # Add blur to image\n",
        "salt = False # Add \"salt\" noise to image\n",
        "noise = False # Add noise to image\n",
        "contrast = False # Add High Contrast to image\n",
        "sharpen = False # Image Sharpen\n",
        "ts = True # Tiling and Scaling\n",
        "augment_affine = False # Affine\n",
        "augment_hsv = False # HSV\n",
        "lr_flip = False # left and right flip\n",
        "ud_flip = False # up and down flip\n",
        "\n",
        "# Training Hyperparameter Related Config\n",
        "momentum = float(0.9)\n",
        "gamma = float(0.95)\n",
        "lr = float(0.001)\n",
        "weight_decay = float(0.0)\n",
        "\n",
        "xy_loss = float(2)\n",
        "wh_loss= float(1.6)\n",
        "no_object_loss = float(25)\n",
        "object_loss = float(0.1)\n",
        "\n",
        "# Debugging/Visualization Related Config\n",
        "debug_mode = False\n",
        "upload_dataset = False\n",
        "vanilla_anchor = False\n",
        "vis_batch = int(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUFcqEoSacfN"
      },
      "source": [
        "Initializing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA2_pqJmacfN",
        "outputId": "22cb0ac7-25d3-48de-bf25-1ea8ce66f7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model\n"
          ]
        }
      ],
      "source": [
        "input_arguments = list(locals().items())\n",
        "\n",
        "print(\"Initializing model\")\n",
        "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKK_EdlPacfO"
      },
      "source": [
        "Processing Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr5ige5CacfO"
      },
      "outputs": [],
      "source": [
        "if output_path == \"automatic\":\n",
        "    current_month = datetime.now().strftime('%B').lower()\n",
        "    current_year = str(datetime.now().year)\n",
        "    if not os.path.exists(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])):\n",
        "        os.makedirs(os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1]))\n",
        "    output_uri = os.path.join('outputs/', current_month + '-' + current_year + '-experiments/' + model_cfg.split('.')[0].split('/')[-1])\n",
        "else:\n",
        "    output_uri = output_path\n",
        "\n",
        "img_width, img_height = model.img_size()\n",
        "bw  = model.get_bw()\n",
        "validate_uri, train_uri = model.get_links()\n",
        "num_validate_images, num_train_images = model.num_images()\n",
        "conf_thresh, nms_thresh, iou_thresh = model.get_threshs()\n",
        "num_classes = model.get_num_classes()\n",
        "loss_constant = model.get_loss_constant()\n",
        "conv_activation = model.get_conv_activation()\n",
        "anchors = model.get_anchors()\n",
        "onnx_name = model.get_onnx_name()\n",
        "\n",
        "start_epoch = 0\n",
        "weights_path = weights_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders"
      ],
      "metadata": {
        "id": "bxHx3Q3Sicab"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAG6cf2acfO"
      },
      "source": [
        "One of our main contributions to vanilla YOLOv3 is the custom data loader we implemented:\n",
        "\n",
        "Each set of training images from a specific sensor/lens/perspective combination is uniformly rescaled such that their landmark size distributions matched that of the camera system on the vehicle. Each training image was then padded if too small or split up into multiple images if too large.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/22118253/69765465-09e90000-1142-11ea-96b7-370868a0033b.png\" width=\"600\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGRk_BEBacfP"
      },
      "outputs": [],
      "source": [
        "with tempfile.TemporaryDirectory() as tensorboard_data_dir:\n",
        "    print(\"Initializing data loaders\")\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        ImageLabelDataset(train_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=augment_hsv,\n",
        "                            augment_affine=augment_affine, num_images=num_train_images,\n",
        "                            bw=bw, n_cpu=num_cpu, lr_flip=lr_flip, ud_flip=ud_flip,vis_batch=vis_batch,data_aug=data_aug,blur=blur,salt=salt,noise=noise,contrast=contrast,sharpen=sharpen,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
        "        batch_size=(1 if debug_mode else batch_size),\n",
        "        shuffle=(False if debug_mode else True),\n",
        "        num_workers=(0 if vis_batch else num_cpu),\n",
        "        pin_memory=cuda)\n",
        "    print(\"Num train images: \", len(train_data_loader.dataset))\n",
        "\n",
        "    validate_data_loader = torch.utils.data.DataLoader(\n",
        "        ImageLabelDataset(validate_uri, dataset_path=dataset_path, width=img_width, height=img_height, augment_hsv=False,\n",
        "                            augment_affine=False, num_images=num_validate_images,\n",
        "                            bw=bw, n_cpu=num_cpu, lr_flip=False, ud_flip=False,vis_batch=vis_batch,data_aug=False,blur=False,salt=False,noise=False,contrast=False,sharpen=False,ts=ts,debug_mode=debug_mode, upload_dataset=upload_dataset),\n",
        "        batch_size=(1 if debug_mode else batch_size),\n",
        "        shuffle=False,\n",
        "        num_workers=(0 if vis_batch else num_cpu),\n",
        "        pin_memory=cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP3sgry_acfP"
      },
      "source": [
        "Initialize Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqn8bjM3acfP"
      },
      "outputs": [],
      "source": [
        "if optimizer_pick == \"Adam\":\n",
        "    print(\"Using Adam Optimizer\")\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                lr=lr, weight_decay=weight_decay)\n",
        "elif optimizer_pick == \"SGD\":\n",
        "    print(\"Using SGD Optimizer\")\n",
        "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                            lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "else:\n",
        "    raise Exception(f\"Invalid optimizer name: {optimizer_pick}\")\n",
        "print(\"Loading weights\")\n",
        "model.load_weights(weights_path, model.get_start_weight_dim())\n",
        "\n",
        "# Set scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUnQ7Br6acfP"
      },
      "source": [
        "Sending Model to GPUs if we are in GPU mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWvYquzKacfP"
      },
      "source": [
        "### Let's Dance (Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4PYPc9YacfP"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    print('Using ', torch.cuda.device_count(), ' GPUs')\n",
        "    model = nn.DataParallel(model)\n",
        "model = model.to(device, non_blocking=True)\n",
        "\n",
        "val_loss = 999  # using a high number for validation loss\n",
        "val_loss_counter = 0\n",
        "step = [0]  # wrapping in an array so it is mutable\n",
        "epoch = start_epoch\n",
        "while epoch < num_epochs and step[0] < num_steps:\n",
        "    epoch += 1\n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    run_epoch(label_prefix=\"train\", data_loader=train_data_loader, epoch=epoch,\n",
        "                step=step, model=model, num_epochs=num_epochs, num_steps=num_steps,\n",
        "                optimizer=optimizer, device=device)\n",
        "    print('Completed epoch: ', epoch)\n",
        "    # Update best loss\n",
        "    if epoch % checkpoint_interval == 0 or epoch == num_epochs or step[0] >= num_steps:\n",
        "        # First, save the weights\n",
        "        save_weights_uri = os.path.join(output_uri, \"{epoch}.weights\".format(epoch=epoch))\n",
        "        model.save_weights(save_weights_uri)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            print(\"Calculating loss on validate data\")\n",
        "            epoch_losses, epoch_time_total, epoch_num_targets = run_epoch(\n",
        "                label_prefix=\"validate\", data_loader=validate_data_loader, epoch=epoch,\n",
        "                model=model, num_epochs=num_epochs, num_steps=num_steps, optimizer=None,\n",
        "                step=step, device=device)\n",
        "            avg_epoch_loss = epoch_losses[0] / epoch_num_targets\n",
        "            print('Average Validation Loss: {0:10.6f}'.format(avg_epoch_loss))\n",
        "\n",
        "            if avg_epoch_loss > val_loss and epoch > min_epochs:\n",
        "                val_loss_counter += 1\n",
        "                print(f\"Validation loss did not decrease for {val_loss_counter}\"\n",
        "                        f\" consecutive check(s)\")\n",
        "            else:\n",
        "                print(\"Validation loss decreased. Yay!!\")\n",
        "                val_loss_counter = 0\n",
        "                val_loss = avg_epoch_loss\n",
        "                ##### updating best result for optuna study #####\n",
        "                result = open(\"logs/result.txt\", \"w\" )\n",
        "                result.write(str(avg_epoch_loss))\n",
        "                result.close() \n",
        "                ###########################################\n",
        "            validate.validate(dataloader=validate_data_loader, model=model, device=device, step=step[0], bbox_all=False,debug_mode=debug_mode)\n",
        "            if val_loss_counter == val_tolerance:\n",
        "                print(\"Validation loss stopped decreasing over the last \" + str(val_tolerance) + \" checkpoints, creating onnx file\")\n",
        "                with tempfile.NamedTemporaryFile() as tmpfile:\n",
        "                    model.save_weights(tmpfile.name)\n",
        "                    weights_name = tmpfile.name\n",
        "                    cfg_name = os.path.join(tempfile.gettempdir(), model_cfg.split('/')[-1].split('.')[0] + '.tmp')\n",
        "                    onnx_gen = subprocess.call(['python3', 'yolo2onnx.py', '--cfg_name', cfg_name, '--weights_name', weights_name])\n",
        "                    save_weights_uri = os.path.join(output_uri, onnx_name)\n",
        "                    os.rename(weights_name, save_weights_uri)\n",
        "                    try:\n",
        "                        os.remove(onnx_name)\n",
        "                        os.remove(cfg_name)\n",
        "                    except:\n",
        "                        pass\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIlu6tPIacfQ"
      },
      "source": [
        "Our full dataset accuracy metrics for detecting traffic cones on the racing track:\n",
        "\n",
        "| mAP | Recall | Precision |\n",
        "|----|----|----|\n",
        "| 89.35% | 92.77% | 86.94% |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxcInOJacfQ"
      },
      "source": [
        "## 3. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUtI9YFEacfQ"
      },
      "source": [
        "Download target video file for inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp -p gs://mit-driverless-open-source/test_yolo_video.mp4 ./\n",
        "# ! wget https://storage.googleapis.com/mit-driverless-open-source/test_yolo_video.mp4\n",
        "\n",
        "! ffmpeg -i test_yolo_video.mp4 test.mp4 && rm test_yolo_video.mp4"
      ],
      "metadata": {
        "id": "6uALFF231Phm",
        "outputId": "a58a8d50-c134-4513-9c36-a327541eb2bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://mit-driverless-open-source/test_yolo_video.mp4...\n",
            "/ [1 files][  2.9 MiB/  2.9 MiB]                                                \n",
            "Operation completed over 1 objects/2.9 MiB.                                      \n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test_yolo_video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 1\n",
            "    compatible_brands: isommp41mp42\n",
            "    creation_time   : 2020-08-29T18:55:33.000000Z\n",
            "  Duration: 00:00:06.01, start: 0.000000, bitrate: 4030 kb/s\n",
            "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], 3882 kb/s, 26 fps, 26 tbr, 13312 tbn, 26 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2020-08-29T18:55:33.000000Z\n",
            "      handler_name    : Core Media Video\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'test.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 1\n",
            "    compatible_brands: isommp41mp42\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x768 [SAR 1:1 DAR 4:3], q=-1--1, 26 fps, 13312 tbn, 26 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2020-08-29T18:55:33.000000Z\n",
            "      handler_name    : Core Media Video\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  157 fps= 18 q=-1.0 Lsize=    1271kB time=00:00:05.92 bitrate=1758.1kbits/s speed=0.672x    \n",
            "video:1269kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.167524%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mframe I:1     Avg QP:20.93  size: 16801\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mframe P:103   Avg QP:23.35  size:  8808\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mframe B:53    Avg QP:25.75  size:  7071\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mconsecutive B-frames: 50.3%  7.6% 19.1% 22.9%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mmb I  I16..4:  9.8% 86.1%  4.0%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mmb P  I16..4:  1.8% 18.7%  0.2%  P16..4: 29.6%  5.3%  1.7%  0.0%  0.0%    skip:42.6%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mmb B  I16..4:  1.3% 11.5%  0.1%  B16..8: 27.7%  5.1%  0.5%  direct: 1.9%  skip:51.9%  L0:56.2% L1:38.2% BI: 5.6%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0m8x8 transform intra:89.8% inter:87.5%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mcoded y,uvDC,uvAC intra: 65.6% 37.4% 4.2% inter: 13.7% 9.7% 0.3%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mi16 v,h,dc,p: 14% 38% 23% 24%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 28% 47%  2%  2%  1%  3%  1%  3%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 34% 16%  3%  7%  6%  9%  3%  5%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mi8c dc,h,v,p: 57% 22% 19%  2%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mref P L0: 63.9% 21.0% 10.1%  5.0%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mref B L0: 86.0% 11.0%  3.0%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mref B L1: 95.9%  4.1%\n",
            "\u001b[1;36m[libx264 @ 0x560266b41900] \u001b[0mkb/s:1720.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6bPReveacfQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_path = 'test.mp4'\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE1k5_8FacfQ"
      },
      "source": [
        "Download pretrained weights for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzpFrJOMacfR",
        "outputId": "0e6040c4-f18d-4fe6-e3ed-62695f863c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://mit-driverless-open-source/pretrained_yolo.weights...\n",
            "\\ [1 files][236.5 MiB/236.5 MiB]                                                \n",
            "Operation completed over 1 objects/236.5 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "! wget https://storage.googleapis.com/mit-driverless-open-source/pretrained_yolo.weights\n",
        "\"\"\"\n",
        "# This should already have been downloaded from the cells above\n",
        "! gsutil cp -p gs://mit-driverless-open-source/pretrained_yolo.weights ./yolo_weights/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_SkJ2LOacfR"
      },
      "source": [
        "Import all packages for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f_UL98m1acfR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import isfile, join\n",
        "import copy\n",
        "import cv2\n",
        "from tensorboardX import SummaryWriter\n",
        "from PIL import Image, ImageDraw\n",
        "import torchvision\n",
        "from utils.nms import nms\n",
        "from utils.utils import calculate_padding\n",
        "from yolo_tutorial_util import single_img_detect, detect\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ThbDTKtoacfR"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "detection_tmp_path = \"/tmp/detect/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jlQcQesacfR"
      },
      "source": [
        "Set up config file for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g4CHMl8kacfR"
      },
      "outputs": [],
      "source": [
        "target_path = \"test.mp4\"\n",
        "output_path = \"outputs/visualization/\"\n",
        "weights_path = \"yolo_weights/pretrained_yolo.weights\"\n",
        "conf_thres = float(0.8)\n",
        "nms_thres = float(0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell to use image instead of video"
      ],
      "metadata": {
        "id": "1EP_Ci_GjGci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_path = \"./dataset/YOLO_Dataset/vid_100_frame_101.jpg\""
      ],
      "metadata": {
        "id": "msWhypt0jLtL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "P63PoML8acfS",
        "outputId": "9226e470-62b1-4ca1-e12b-8d8a0f5e5465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection Mode is: image\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1afafbbd6c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf_thres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnms_thres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_tmp_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_tmp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/yolo_tutorial_util.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(target_path, output_path, model, device, conf_thres, nms_thres, detection_tmp_path)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mdetection_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_img_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_filepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf_thres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnms_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnms_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Please check output image at {detection_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolo_tutorial_util.py\u001b[0m in \u001b[0;36msingle_img_detect\u001b[0;34m(target_path, output_path, mode, model, device, conf_thres, nms_thres)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_box_corner\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpad_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_box_corner\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpad_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mcropped_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_with_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mcropped_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcones_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_cones_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'crop'"
          ]
        }
      ],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if cuda else 'cpu')\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.cuda.empty_cache()\n",
        "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
        "\n",
        "# Load weights\n",
        "model.load_weights(weights_path, model.get_start_weight_dim())\n",
        "model.to(device, non_blocking=True)\n",
        "\n",
        "detect(target_path, output_path, model, device=device, conf_thres=conf_thres, nms_thres=nms_thres, detection_tmp_path=detection_tmp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "K7sFJx3HacfS"
      },
      "outputs": [],
      "source": [
        "! cd outputs/visualization/ && ffmpeg -i test.mp4 output.mp4 && rm test.mp4 && cd ../..\n",
        "\n",
        "video_path = \"outputs/visualization/output.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTr4ARRxacfS"
      },
      "source": [
        "**Notice:** Again, you can further improve the accuracy of the cone detection network by switching YOLOv3 backbone to the most recent published YOLOv4\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/22118253/70950893-e2de6980-202f-11ea-9a16-399579926ee5.gif\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "Congratulations! You've finished all the content of this tutorial!\n",
        "Hope you enjoy playing with the our object detection model. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
        "\n",
        "## Reference\n",
        "[1] Kieran Strobel, Sibo Zhu, Raphael Chang and Skanda Koppula.\n",
        "**Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions**. In *IROS* 2020.\n",
        "[[paper]](https://arxiv.org/abs/2007.13971), [[code]](https://github.com/cv-core/MIT-Driverless-CV-TrainingInfra)."
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python36964bitfb145c69a41e49ec9393ba0ede4656b6"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "yolo_tutorial.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}