{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import math\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import Darknet\n",
    "from utils.datasets import ImageLabelDataset\n",
    "from utils.utils import model_info, print_args, Logger, visualize_and_save_to_local,xywh2xyxy\n",
    "import validate\n",
    "import warnings\n",
    "\n",
    "import sys\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import cv2\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "from utils.nms import nms\n",
    "from utils.utils import calculate_padding\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Code added\n",
    "# from crop_image import crop_image\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "num_cpu = multiprocessing.cpu_count() if cuda else 0\n",
    "\n",
    "def run_epoch(label_prefix, data_loader, num_steps, optimizer, model, epoch, num_epochs, step, device):\n",
    "    print(f\"Model in {label_prefix} mode\")\n",
    "    epoch_losses = [0.0] * 7\n",
    "    epoch_time_total = 0.0\n",
    "    epoch_num_targets = 1e-12\n",
    "    t1 = time.time()\n",
    "    loss_labels = [\"Total\", \"L-x\", \"L-y\", \"L-w\", \"L-h\", \"L-noobj\", \"L-obj\"]\n",
    "    for i, (img_uri, imgs, targets) in enumerate(data_loader):\n",
    "        if step[0] >= num_steps:\n",
    "            break\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        targets.requires_grad_(False)\n",
    "        step_num_targets = ((targets[:, :, 1:5] > 0).sum(dim=2) > 1).sum().item() + 1e-12\n",
    "        epoch_num_targets += step_num_targets\n",
    "        # Compute loss, compute gradient, update parameters\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "        losses = model(imgs, targets)\n",
    "        if label_prefix == \"train\":\n",
    "            losses[0].sum().backward()\n",
    "        if optimizer is not None:\n",
    "            optimizer.step()\n",
    "\n",
    "        for j, (label, loss) in enumerate(zip(loss_labels, losses)):\n",
    "            batch_loss = loss.sum().to('cpu').item()\n",
    "            epoch_losses[j] += batch_loss\n",
    "        finished_time = time.time()\n",
    "        step_time_total = finished_time - t1\n",
    "        epoch_time_total += step_time_total\n",
    "        \n",
    "        statement = label_prefix + ' Epoch: ' + str(epoch) + ', Batch: ' + str(i + 1) + '/' + str(len(data_loader))\n",
    "        count = 0\n",
    "        for (loss_label, loss) in zip(loss_labels, losses):\n",
    "            if count == 0:\n",
    "                statement += ', Total: ' + '{0:10.6f}'.format(loss.item() / step_num_targets)\n",
    "                tot_loss = loss.item()\n",
    "                count += 1\n",
    "            else:\n",
    "                statement += ',   ' + loss_label + ': {0:5.2f}'.format(loss.item() / tot_loss * 100) + '%'\n",
    "        print(statement)\n",
    "        if label_prefix == \"train\":\n",
    "            step[0] += 1\n",
    "    return epoch_losses, epoch_time_total, epoch_num_targets\n",
    "\n",
    "\n",
    "def single_img_detect(target_path,output_path,mode,model,device,conf_thres,nms_thres):\n",
    "    \"\"\"\n",
    "    Saves:\n",
    "        img_with_boxes: An image with rectangles showing where the cone is.\n",
    "    \"\"\"\n",
    "    img = Image.open(target_path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    new_width, new_height = model.img_size()\n",
    "    pad_h, pad_w, ratio = calculate_padding(h, w, new_height, new_width)\n",
    "    # Add padding to the image\n",
    "    img = torchvision.transforms.functional.pad(img, padding=(pad_w, pad_h, pad_w, pad_h), fill=(127, 127, 127), padding_mode=\"constant\")\n",
    "    # Resize the image\n",
    "    img = torchvision.transforms.functional.resize(img, (new_height, new_width))\n",
    "\n",
    "    # Check if the model needs the input image to be Black and White\n",
    "    bw = model.get_bw()\n",
    "    if bw:\n",
    "        img = torchvision.transforms.functional.to_grayscale(img, num_output_channels=1)\n",
    "\n",
    "    img = torchvision.transforms.functional.to_tensor(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        # output,first_layer,second_layer,third_layer = model(img)\n",
    "        # TODO: What does the model (Darknet) output (the format)?\n",
    "        output = model(img)\n",
    "\n",
    "        for detections in output:\n",
    "            detections = detections[detections[:, 4] > conf_thres]\n",
    "            box_corner = torch.zeros((detections.shape[0], 4), device=detections.device)\n",
    "            xy = detections[:, 0:2]\n",
    "            wh = detections[:, 2:4] / 2\n",
    "            box_corner[:, 0:2] = xy - wh\n",
    "            box_corner[:, 2:4] = xy + wh\n",
    "            probabilities = detections[:, 4]\n",
    "            nms_indices = nms(box_corner, probabilities, nms_thres)\n",
    "            main_box_corner = box_corner[nms_indices]\n",
    "            if nms_indices.shape[0] == 0:  \n",
    "                continue\n",
    "        img_with_boxes = Image.open(target_path)\n",
    "        draw = ImageDraw.Draw(img_with_boxes)\n",
    "        w, h = img_with_boxes.size\n",
    "\n",
    "        d = 1\n",
    "        # Draw the rectangles in the images and save images of cones\n",
    "        for i in range(len(main_box_corner)):\n",
    "            # target_cones_path = f\"cones_{d}.jpg\"\n",
    "            target_cones_path = \"cone.jpg\"\n",
    "            cones_path = \"outputs/cones/\"\n",
    "            x0 = main_box_corner[i, 0].to('cpu').item() / ratio - pad_w\n",
    "            y0 = main_box_corner[i, 1].to('cpu').item() / ratio - pad_h\n",
    "            x1 = main_box_corner[i, 2].to('cpu').item() / ratio - pad_w\n",
    "            y1 = main_box_corner[i, 3].to('cpu').item() / ratio - pad_h\n",
    "            cropped_image = img_with_boxes.crop((x0, y0, x1, y1))\n",
    "            cropped_image.save(os.path.join(cones_path, target_cones_path.split('/')[-1]))\n",
    "            draw.rectangle((x0, y0, x1, y1), outline=\"red\")\n",
    "            d += 1\n",
    "        print(\"Images have been cropped\")\n",
    "\n",
    "        if mode == 'image':\n",
    "            # Save the images with boxes drawn\n",
    "            img_with_boxes.save(os.path.join(output_path,target_path.split('/')[-1]))\n",
    "            return os.path.join(output_path,target_path.split('/')[-1])\n",
    "        else:\n",
    "            img_with_boxes.save(target_path)\n",
    "            return target_path\n",
    "\n",
    "def detect(target_path,\n",
    "           output_path,\n",
    "           model,\n",
    "           device,\n",
    "           conf_thres,\n",
    "           nms_thres,\n",
    "           detection_tmp_path):\n",
    "\n",
    "        target_filepath = target_path\n",
    "\n",
    "        img_formats = ['.jpg', '.jpeg', '.png', '.tif']\n",
    "        vid_formats = ['.mov', '.avi', '.mp4']\n",
    "\n",
    "        mode = None\n",
    "\n",
    "        # Find the type of file we are dealing with\n",
    "        if os.path.splitext(target_filepath)[-1].lower() in img_formats:\n",
    "            mode = 'image'\n",
    "        elif os.path.splitext(target_filepath)[-1].lower() in vid_formats:\n",
    "            mode = 'video'\n",
    "        print(\"Detection Mode is: \" + mode)\n",
    "\n",
    "        raw_file_name = target_filepath.split('/')[-1].split('.')[0].split('_')[-4:]\n",
    "        raw_file_name = '_'.join(raw_file_name)\n",
    "        \n",
    "        if mode == 'image':\n",
    "            detection_path = single_img_detect(target_path=target_filepath,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "\n",
    "            print(f'Please check output image at {detection_path}')\n",
    "\n",
    "        elif mode == 'video':\n",
    "            # Create a temporary path for storing the frames where the output is detected.\n",
    "            if os.path.exists(detection_tmp_path):\n",
    "                shutil.rmtree(detection_tmp_path)  # delete output folder\n",
    "            os.makedirs(detection_tmp_path)  # make new output folder\n",
    "\n",
    "            # Grabs and returns the next video frame\n",
    "            vidcap = cv2.VideoCapture(target_filepath)\n",
    "            success,image = vidcap.read()\n",
    "            count = 0\n",
    "\n",
    "            # If there is a next frame, store the frame in temp path\n",
    "            while success:\n",
    "                cv2.imwrite(detection_tmp_path + \"/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "                success,image = vidcap.read()\n",
    "                count += 1\n",
    "\n",
    "            # Find OpenCV version\n",
    "            (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "            if int(major_ver)< 3:\n",
    "                fps = vidcap.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "                print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "            else:\n",
    "                fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "                print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "            vidcap.release();  # Closes the video file\n",
    "\n",
    "            frame_array = []\n",
    "            files = [f for f in os.listdir(detection_tmp_path) if isfile(join(detection_tmp_path, f))]\n",
    "        \n",
    "            # Detects all the cones in the frames\n",
    "            files.sort(key = lambda x: int(x[5:-4])) # for sorting the file names properly\n",
    "            for i in tqdm(files,desc='Doing Single Image Detection'):\n",
    "                filename=detection_tmp_path + i\n",
    "                \n",
    "                detection_path = single_img_detect(target_path=filename,output_path=output_path,mode=mode,model=model,device=device,conf_thres=conf_thres,nms_thres=nms_thres)\n",
    "                #reading each files\n",
    "                img = cv2.imread(detection_path)\n",
    "                height, width, layers = img.shape\n",
    "                size = (width,height)\n",
    "                frame_array.append(img)\n",
    "\n",
    "            # Compiles all the frames together to create a video\n",
    "            local_output_uri = output_path + raw_file_name + \".mp4\"\n",
    "            video_output = cv2.VideoWriter(local_output_uri,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "            for frame in tqdm(frame_array,desc='Creating Video'):\n",
    "                # writing to a image array\n",
    "                video_output.write(frame)\n",
    "            video_output.release()\n",
    "            shutil.rmtree(detection_tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"./sample_image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60537b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "model = Darknet(config_path=model_cfg,xy_loss=xy_loss,wh_loss=wh_loss,no_object_loss=no_object_loss,object_loss=object_loss,vanilla_anchor=vanilla_anchor)\n",
    "\n",
    "# Load weights\n",
    "model.load_weights(weights_path, model.get_start_weight_dim())\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "detect(target_path, output_path, model, device=device, conf_thres=conf_thres, nms_thres=nms_thres, detection_tmp_path=detection_tmp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
